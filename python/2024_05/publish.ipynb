{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pika datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import datasets\n",
    "dl_manager = datasets.DownloadManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# common iterator pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def iter_mode_1(dataset):\n",
    "    for row in dataset:\n",
    "        text = \"\"\n",
    "        for data in row[\"段落\"]:\n",
    "            text += data[\"内容\"]\n",
    "        yield text\n",
    "        \n",
    "def iter_mode_qa(dataset):\n",
    "    for row in dataset:\n",
    "        text = f'{row[\"问\"]}{row[\"答\"]}'\n",
    "        yield text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# publish tool\n",
    "publish data to rabbitmq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pika\n",
    "import tqdm\n",
    "import logging\n",
    "import time\n",
    "\n",
    "def setup_logging():\n",
    "    formatter = logging.Formatter(fmt='%(asctime)s %(levelname)-8s %(message)s',\n",
    "                                  datefmt='%Y-%m-%d %H:%M:%S')\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(formatter)\n",
    "    logger = logging.getLogger('my_module')\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logger.addHandler(handler)\n",
    "    return logger\n",
    "\n",
    "logger = setup_logging()\n",
    "\n",
    "\n",
    "def get_rabbit_channel():\n",
    "    connection = pika.BlockingConnection(pika.URLParameters('amqp://user:password@hz.knogen.com/%2f'))\n",
    "    connection.process_data_events(time_limit=5)\n",
    "    pk_channel = connection.channel()\n",
    "    # pk_channel.is_closed()\n",
    "    return pk_channel\n",
    "\n",
    "\n",
    "# 测试大小上限为 16GB\n",
    "SIZE_LIMIT = 16 * 1024 * 1024 * 1024 \n",
    "\n",
    "def publish_data(channel_name, iterator_func):\n",
    "    \n",
    "    pk_channel = get_rabbit_channel()\n",
    "    pk_channel.queue_declare(queue=channel_name)\n",
    "    file_size_current = 0\n",
    "    for data in tqdm.tqdm(iterator_func()):\n",
    "        while 1:\n",
    "            try:\n",
    "                file_size_current += len(data)\n",
    "                pk_channel.basic_publish(exchange='', routing_key=channel_name, body=data)\n",
    "                if file_size_current > SIZE_LIMIT:\n",
    "                    return\n",
    "                break\n",
    "            except (pika.exceptions.ChannelClosed,pika.exceptions.ChannelWrongStateError,pika.exceptions.StreamLostError):\n",
    "                logger.info(\"ChannelClosed\")\n",
    "                time.sleep(1)\n",
    "                pk_channel = get_rabbit_channel()\n",
    "            except Exception as e:\n",
    "                logger.exception(e)\n",
    "                break\n",
    "    pk_channel.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# publish MNBVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnbvc_data():\n",
    "    dataset = load_dataset(\"liwu/MNBVC\", 'gov_xuexiqiangguo', split='train', streaming=True,trust_remote_code=True)\n",
    "    for i, row in enumerate(iter_mode_1(dataset)):\n",
    "        if i < 3:\n",
    "            print(i,row)\n",
    "        yield row\n",
    "\n",
    "publish_data(\"gov_xuexiqiangguo\", get_mnbvc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnbvc_data():\n",
    "    dataset = load_dataset(\"liwu/MNBVC\", 'gov_xuexiqiangguo', split='train', streaming=True,trust_remote_code=True)\n",
    "    for i, row in enumerate(iter_mode_1(dataset)):\n",
    "        if i < 1:\n",
    "            print(i,row)\n",
    "        yield row.encode(\"GBK\",errors=\"ignore\")\n",
    "\n",
    "publish_data(\"gov_xuexiqiangguo_gbk\", get_mnbvc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnbvc_data():\n",
    "    dataset = load_dataset(\"liwu/MNBVC\", 'qa_zhihu', split='train', streaming=True,trust_remote_code=True)\n",
    "    for i, row in enumerate(iter_mode_qa(dataset)):\n",
    "        if i < 3:\n",
    "            print(i,row)\n",
    "        yield row\n",
    "\n",
    "publish_data(\"qa_zhihu\", get_mnbvc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnbvc_data():\n",
    "    dataset = load_dataset(\"liwu/MNBVC\", 'qa_zhihu', split='train', streaming=True,trust_remote_code=True)\n",
    "    for i, row in enumerate(iter_mode_qa(dataset)):\n",
    "        if i < 1:\n",
    "            print(i,row)\n",
    "        yield row.encode(\"GBK\",errors=\"ignore\")\n",
    "\n",
    "publish_data(\"qa_zhihu_gbk\", get_mnbvc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnbvc_data():\n",
    "    dataset = load_dataset(\"liwu/MNBVC\", 'qa_mfa', split='train', streaming=True,trust_remote_code=True)\n",
    "    for i, row in enumerate(iter_mode_qa(dataset)):\n",
    "        if i < 1:\n",
    "            print(i,row)\n",
    "        yield row\n",
    "\n",
    "publish_data(\"qa_mfa\", get_mnbvc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnbvc_data():\n",
    "    dataset = load_dataset(\"liwu/MNBVC\", 'qa_mfa', split='train', streaming=True,trust_remote_code=True)\n",
    "    for i, row in enumerate(iter_mode_qa(dataset)):\n",
    "        if i < 1:\n",
    "            print(i,row)\n",
    "        yield row.encode(\"GBK\",errors=\"ignore\")\n",
    "\n",
    "publish_data(\"qa_mfa_gbk\", get_mnbvc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnbvc_data():\n",
    "    dataset = load_dataset(\"liwu/MNBVC\", 'qa_chatgpt', split='train', streaming=True,trust_remote_code=True)\n",
    "    for i, row in enumerate(iter_mode_qa(dataset)):\n",
    "        if i < 3:\n",
    "            print(i,row)\n",
    "        yield row\n",
    "\n",
    "publish_data(\"qa_chatgpt\", get_mnbvc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnbvc_data():\n",
    "    dataset = load_dataset(\"liwu/MNBVC\", 'crawler_oscar', split='train', streaming=True,trust_remote_code=True)\n",
    "    for i, row in enumerate(iter_mode_1(dataset)):\n",
    "        if i < 3:\n",
    "            print(i,row)\n",
    "        yield row\n",
    "\n",
    "publish_data(\"crawler_oscar\", get_mnbvc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_poetry_data():\n",
    "    dataset = load_dataset(\"erhwenkuo/poetry-chinese-zhtw\", split=\"train\")\n",
    "    for i, row in enumerate(dataset):\n",
    "        if row['category'] == \"五代十國\":\n",
    "            text = f\"{row['title']} {row['author']} {row['text']}\"\n",
    "            if i <3:\n",
    "                print(text)\n",
    "            yield text\n",
    "\n",
    "publish_data(\"poetry_wdsg\", get_poetry_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_poetry_data():\n",
    "    dataset = load_dataset(\"erhwenkuo/poetry-chinese-zhtw\", split=\"train\")\n",
    "    for i, row in enumerate(dataset):\n",
    "        if row['category'] == \"唐\":\n",
    "            text = f\"{row['title']} {row['author']} {row['text']}\"\n",
    "            if i <3:\n",
    "                print(text)\n",
    "            yield text\n",
    "\n",
    "publish_data(\"poetry_t\", get_poetry_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_poetry_data():\n",
    "    dataset = load_dataset(\"erhwenkuo/poetry-chinese-zhtw\", split=\"train\")\n",
    "    for i, row in enumerate(dataset):\n",
    "        if row['category'] == \"宋\":\n",
    "            text = f\"{row['title']} {row['author']} {row['text']}\"\n",
    "            if i <3:\n",
    "                print(text)\n",
    "            yield text\n",
    "\n",
    "publish_data(\"poetry_s\", get_poetry_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_poetry_data():\n",
    "    dataset = load_dataset(\"erhwenkuo/poetry-chinese-zhtw\", split=\"train\")\n",
    "    for i, row in enumerate(dataset):\n",
    "        if row['category'] == \"元\":\n",
    "            text = f\"{row['title']} {row['author']} {row['text']}\"\n",
    "            if i <3:\n",
    "                print(text)\n",
    "            yield text\n",
    "\n",
    "publish_data(\"poetry_y\", get_poetry_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_poetry_data():\n",
    "    dataset = load_dataset(\"erhwenkuo/poetry-chinese-zhtw\", split=\"train\")\n",
    "    for i, row in enumerate(dataset):\n",
    "        if row['category'] == \"清\":\n",
    "            text = f\"{row['title']} {row['author']} {row['text']}\"\n",
    "            if i <3:\n",
    "                print(text)\n",
    "            yield text\n",
    "\n",
    "publish_data(\"poetry_q\", get_poetry_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_poetry_data():\n",
    "    dataset = load_dataset(\"Iess/chinese_modern_poetry\", split=\"train\")\n",
    "    for i, row in enumerate(dataset):\n",
    "        text = row[\"response\"]\n",
    "        if i <3:\n",
    "            print(text)\n",
    "        yield text\n",
    "\n",
    "publish_data(\"poetry_modern\", get_poetry_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# publish gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import gzip\n",
    "dl_manager = datasets.DownloadManager()\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_fn(char: str) -> str:\n",
    "    \"\"\"\n",
    "    Transforms any letter different from a base nucleotide into an 'N'.\n",
    "    \"\"\"\n",
    "    if char in {'A', 'T', 'C', 'G'}:\n",
    "        return char\n",
    "    else:\n",
    "        return 'A'\n",
    "\n",
    "\n",
    "def clean_sequence(seq: str) -> str:\n",
    "    \"\"\"\n",
    "    Process a chunk of DNA to have all letters in upper and restricted to\n",
    "    A, T, C, G and N.\n",
    "    \"\"\"\n",
    "    seq = seq.upper()\n",
    "    seq = map(filter_fn, seq)\n",
    "    seq = ''.join(list(seq))\n",
    "    return seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_genoe():\n",
    "    url = \"https://ftp.ncbi.nlm.nih.gov/genomes/refseq/vertebrate_mammalian/Homo_sapiens/latest_assembly_versions/GCF_000001405.40_GRCh38.p14/GCF_000001405.40_GRCh38.p14_genomic.fna.gz\"\n",
    "    file = dl_manager.download(url)\n",
    "\n",
    "    with gzip.open(file,'rt')as f:\n",
    "        fasta_sequences = SeqIO.parse(f, 'fasta')\n",
    "\n",
    "        for record in fasta_sequences:\n",
    "\n",
    "            sequence, description = str(record.seq), record.description\n",
    "            sequence = clean_sequence(sequence)\n",
    "            seq_length = len(sequence)\n",
    "\n",
    "            chunk_length = 1_000_000\n",
    "            # split into chunks\n",
    "            num_chunks = seq_length  // chunk_length\n",
    "\n",
    "            # if num_chunks < 1:\n",
    "            #     continue\n",
    "\n",
    "            for i in range(num_chunks+1):\n",
    "                start_pos = i * chunk_length\n",
    "                end_pos = min(seq_length, (i+1) * chunk_length)\n",
    "                chunk_sequence = sequence[start_pos:end_pos]\n",
    "                yield chunk_sequence\n",
    "        \n",
    "publish_data(\"genoe_human\", get_genoe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_genoe():\n",
    "    url = \"https://ftp.ncbi.nlm.nih.gov/genomes/refseq/vertebrate_mammalian/Hylobates_moloch/latest_assembly_versions/GCF_009828535.3_HMol_V3/GCF_009828535.3_HMol_V3_genomic.fna.gz\"\n",
    "    file = dl_manager.download(url)\n",
    "\n",
    "    with gzip.open(file,'rt')as f:\n",
    "        fasta_sequences = SeqIO.parse(f, 'fasta')\n",
    "\n",
    "        for record in fasta_sequences:\n",
    "\n",
    "            sequence, description = str(record.seq), record.description\n",
    "            sequence = clean_sequence(sequence)\n",
    "            seq_length = len(sequence)\n",
    "\n",
    "            chunk_length = 1_000_000\n",
    "            # split into chunks\n",
    "            num_chunks = seq_length  // chunk_length\n",
    "\n",
    "            # if num_chunks < 1:\n",
    "            #     continue\n",
    "\n",
    "            for i in range(num_chunks +1):\n",
    "                start_pos = i * chunk_length\n",
    "                end_pos = min(seq_length, (i+1) * chunk_length)\n",
    "                chunk_sequence = sequence[start_pos:end_pos]\n",
    "                yield chunk_sequence\n",
    "        \n",
    "publish_data(\"genoe_cby\", get_genoe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_genoe():\n",
    "    url = \"https://ftp.ncbi.nlm.nih.gov/genomes/refseq/bacteria/Robbsia_andropogonis/latest_assembly_versions/GCF_000566705.1_Ba3549/GCF_000566705.1_Ba3549_genomic.fna.gz\"\n",
    "    file = dl_manager.download(url)\n",
    "\n",
    "    with gzip.open(file,'rt')as f:\n",
    "        fasta_sequences = SeqIO.parse(f, 'fasta')\n",
    "\n",
    "        for record in fasta_sequences:\n",
    "\n",
    "            sequence, description = str(record.seq), record.description\n",
    "            sequence = clean_sequence(sequence)\n",
    "            seq_length = len(sequence)\n",
    "\n",
    "            chunk_length = 1_000_000\n",
    "            # split into chunks\n",
    "            num_chunks = seq_length  // chunk_length\n",
    "\n",
    "            # if num_chunks < 1:\n",
    "            #     continue\n",
    "\n",
    "            for i in range(num_chunks+1):\n",
    "                start_pos = i * chunk_length\n",
    "                end_pos = min(seq_length, (i+1) * chunk_length)\n",
    "                chunk_sequence = sequence[start_pos:end_pos]\n",
    "                yield chunk_sequence\n",
    "        \n",
    "publish_data(\"genoe_xj\", get_genoe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_genoe():\n",
    "    url = \"https://ftp.ncbi.nlm.nih.gov/genomes/refseq/fungi/Cladophialophora_bantiana/latest_assembly_versions/GCF_000835475.1_Clad_bant_CBS_173_52_V1/GCF_000835475.1_Clad_bant_CBS_173_52_V1_genomic.fna.gz\"\n",
    "    file = dl_manager.download(url)\n",
    "\n",
    "    with gzip.open(file,'rt')as f:\n",
    "        fasta_sequences = SeqIO.parse(f, 'fasta')\n",
    "\n",
    "        for record in fasta_sequences:\n",
    "\n",
    "            sequence, description = str(record.seq), record.description\n",
    "            sequence = clean_sequence(sequence)\n",
    "            seq_length = len(sequence)\n",
    "\n",
    "            chunk_length = 1_000_000\n",
    "            # split into chunks\n",
    "            num_chunks = seq_length  // chunk_length\n",
    "\n",
    "            # if num_chunks < 1:\n",
    "            #     continue\n",
    "\n",
    "            for i in range(num_chunks+1):\n",
    "                start_pos = i * chunk_length\n",
    "                end_pos = min(seq_length, (i+1) * chunk_length)\n",
    "                chunk_sequence = sequence[start_pos:end_pos]\n",
    "                yield chunk_sequence\n",
    "        \n",
    "publish_data(\"genoe_zj\", get_genoe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 无理数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pi upload 10B¶\n",
    "import datasets\n",
    "import zipfile\n",
    "\n",
    "def read_zip_file(file):\n",
    "    \n",
    "    B = 1_000_000\n",
    "    with zipfile.ZipFile(file,'r') as myzip:\n",
    "        for fname in myzip.namelist():\n",
    "            with myzip.open(fname,\"r\")as f:\n",
    "                data = f.read()\n",
    "                for i in range(0,10):\n",
    "                    start = i*B\n",
    "                    end = i*B + B\n",
    "                    yield data[start:end]\n",
    "                \n",
    "def get_pi_text():\n",
    "    for i in range(0,10):\n",
    "        URL = f\"https://files.pilookup.com/pi/{i*100_000_000 + 1}-{i*100_000_000+100_000_000}.zip\"\n",
    "        file = dl_manager.download(URL)\n",
    "        yield from read_zip_file(file)\n",
    "        \n",
    "\n",
    "publish_data(\"num_pi\", get_pi_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EulersNumberE upload 1B\n",
    "url= \"https://archive.org/download/EulersNumberE7.5BillionDigits/part1.txt\"\n",
    "file = dl_manager.download(url)\n",
    "\n",
    "def get_e_text():\n",
    "    url= \"https://archive.org/download/EulersNumberE7.5BillionDigits/part1.txt\"\n",
    "    file = dl_manager.download(url)\n",
    "    B = 1_000_000\n",
    "    with open(file,'rt')as f:\n",
    "        data = f.read()\n",
    "    \n",
    "        for i in range(0,100):\n",
    "            start = i*B + 2\n",
    "            end = i*B + 2 + B\n",
    "            yield data[start:end]\n",
    "        \n",
    "\n",
    "publish_data(\"num_e\", get_e_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# shakespeares\n",
    "\n",
    "download a zip file that contain multiple shakespeares works\n",
    "\n",
    "website: https://www.folger.edu/explore/shakespeares-works/download/\n",
    "download url: https://flgr.sh/txtfssAlltxt"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
